{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3D_Classification_Dict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1Ahc6QaOCoako8m39k6yJEwF0n_OpNIkw",
      "authorship_tag": "ABX9TyMY6ky3yMNm/3WtAzXplGGL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meddebma/pyradiomics/blob/master/3D_Classification_Dict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiFnM05m6uL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce2a5692-87be-4296-ea3e-3e51606a818b"
      },
      "source": [
        "%pip install -q \"monai-weekly[itk, pillow]\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 573kB 4.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 36.3MB 133kB/s \n",
            "\u001b[K     |████████████████████████████████| 10.3MB 42.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 14.0MB 24.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 68.0MB 50kB/s \n",
            "\u001b[K     |████████████████████████████████| 14.4MB 24.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 50.1MB 91kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JIt-TFM-3ZL",
        "outputId": "8eb4adcf-63e2-4f65-c7c8-edecc42422df"
      },
      "source": [
        "# Copyright 2020 MONAI Consortium\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import pandas as pd\n",
        "import seaborn as sns \n",
        "from glob import glob\n",
        "\n",
        "import monai\n",
        "from monai.metrics import compute_roc_auc\n",
        "from monai.networks.nets import DenseNet121, Classifier\n",
        "from monai.apps import download_and_extract\n",
        "from monai.config import print_config\n",
        "from monai.utils import first\n",
        "from monai.visualize import GradCAM\n",
        "from monai.metrics import ConfusionMatrixMetric, get_confusion_matrix\n",
        "from monai.data import CacheDataset, DataLoader, ImageDataset, ITKReader, PILReader, Dataset, partition_dataset_classes\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    ConfusionMatrixDisplay, \n",
        ")\n",
        "from monai.transforms import ScaleIntensity, Rotate90d, Orientationd, Flipd, AddChannel, Resize, ToTensor, Activations, Activationsd, AsDiscreted, AddChanneld, Invertd, AsDiscrete, Compose, LoadImaged, RandRotate90d, Resized, ScaleIntensityd, ToTensord\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "print_config()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MONAI version: 0.6.dev2126\n",
            "Numpy version: 1.19.5\n",
            "Pytorch version: 1.9.0+cu102\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
            "MONAI rev id: 2ad54662de25e9a964c33327f7f2f178655573ef\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 3.0.2\n",
            "scikit-image version: 0.16.2\n",
            "Pillow version: 7.1.2\n",
            "Tensorboard version: 2.5.0\n",
            "gdown version: 3.6.4\n",
            "TorchVision version: 0.10.0+cu102\n",
            "ITK version: 5.1.2\n",
            "tqdm version: 4.41.1\n",
            "lmdb version: 0.99\n",
            "psutil version: 5.4.8\n",
            "pandas version: 1.1.5\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvUL9l6t_tDM"
      },
      "source": [
        "**Data Direction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHeS8Rt6_zWD"
      },
      "source": [
        "data_dir= \"/content/drive/My Drive/Spleen_AI\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1S2v-B-AITM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2dcf725-17d3-4502-8355-33cb24813aa5"
      },
      "source": [
        "#labels_all = pd.read_excel (r'/content/drive/MyDrive/Spleen_AI/labels.xlsx')\n",
        "#l=labels_all[\"label\"].to_numpy()\n",
        "#print(l)\n",
        "#print(labels_all[\"label\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyjyR96rS0Bl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb649d0-98f4-42c4-eba8-845dfe58a2ca"
      },
      "source": [
        "images = sorted(glob(os.path.join(data_dir, \"Projekt2t/Segmentation/*\", \"*.nii.gz\")))\n",
        "\n",
        "for i in images:\n",
        "  print(os.path.basename(i))\n",
        "\n",
        "\n",
        "print(len(images)) \n",
        "\n",
        "\n",
        "#for i in labels_all:\n",
        "#  print (labels_all[i])\n",
        "#labels= l\n",
        "labels= np.array([0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1], dtype=np.int64)\n",
        "print(len(labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12656178_seg.nii.gz\n",
            "12706474_seg.nii.gz\n",
            "12706553_seg.nii.gz\n",
            "12706591_seg.nii.gz\n",
            "12714971_seg.nii.gz\n",
            "12734777_seg.nii.gz\n",
            "12737076_seg.nii.gz\n",
            "12738318_seg.nii.gz\n",
            "12744614_seg.nii.gz\n",
            "12746064_seg.nii.gz\n",
            "12750465_seg.nii.gz\n",
            "12750630_seg.nii.gz\n",
            "12751960_seg.nii.gz\n",
            "13151781_seg.nii.gz\n",
            "13154242_seg.nii.gz\n",
            "13155618_seg.nii.gz\n",
            "13166298_seg.nii.gz\n",
            "13166424_seg.nii.gz\n",
            "13196467_seg.nii.gz\n",
            "13206640_seg.nii.gz\n",
            "13225356_seg.nii.gz\n",
            "13229161_seg.nii.gz\n",
            "13235081_seg.nii.gz\n",
            "23\n",
            "23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJJwFbO47yGV"
      },
      "source": [
        "**Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ensxgO2N37j"
      },
      "source": [
        "train_files = [{\"img\": img, \"label\": label} for img, label in zip(images[:18], labels[:18])]\n",
        "val_files = [{\"img\": img, \"label\": label} for img, label in zip(images[-18:], labels[-18:])]\n",
        "\n",
        "    # Define transforms for image\n",
        "train_transforms = Compose(\n",
        "        [\n",
        "            LoadImaged(keys=[\"img\"]),\n",
        "            AddChanneld(keys=[\"img\"]),\n",
        "            ScaleIntensityd(keys=[\"img\"]),\n",
        "            #Orientationd(keys=[\"img\"],axcodes=\"PRI\"),\n",
        "            #Rotate90d(keys=[\"img\"], k=3, spatial_axes=(1, 2)),\n",
        "            #Flipd(keys=[\"img\"], spatial_axis=0),\n",
        "            Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),\n",
        "            #RandRotate90d(keys=[\"img\"], prob=1, spatial_axes=[0, 2]),\n",
        "            ToTensord(keys=[\"img\"]),\n",
        "        ]\n",
        "    )\n",
        "val_transforms = Compose(\n",
        "        [\n",
        "            LoadImaged(keys=[\"img\"]),\n",
        "            AddChanneld(keys=[\"img\"]),\n",
        "            ScaleIntensityd(keys=[\"img\"]),\n",
        "            #Orientationd(keys=[\"img\"],axcodes=\"PRI\"),\n",
        "            #Rotate90d(keys=[\"img\"], k=3, spatial_axes=(1, 2)),\n",
        "            #Flipd(keys=[\"img\"], spatial_axis=0),\n",
        "            #RandRotate90d(keys=[\"img\"], prob=1, spatial_axes=[1, 2]),\n",
        "            Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),\n",
        "            ToTensord(keys=[\"img\"]),\n",
        "        ]\n",
        "    )\n",
        "act = Activations(softmax=True)\n",
        "to_onehot = AsDiscrete(to_onehot=True, n_classes=2)\n",
        "\n",
        "\n",
        "# Define dataset, data loader\n",
        "check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
        "check_loader = DataLoader(check_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n",
        "#check_data = monai.utils.misc.first(check_loader)\n",
        "#print(check_data[\"img\"].shape, check_data[\"label\"])\n",
        "\n",
        "# create a training data loader\n",
        "train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "# create a validation data loader\n",
        "val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "post_transforms = Compose([\n",
        "        #Activationsd(keys=\"y_pred\", sigmoid=True),\n",
        "        #AsDiscreted(keys=\"y_pred\", threshold_values=True),\n",
        "        Invertd(\n",
        "            keys=[\"img\"],  # invert the `pred` data field, also support multiple fields\n",
        "            transform=val_transforms,\n",
        "            loader=val_loader,\n",
        "            orig_keys=\"img\",  # get the previously applied pre_transforms information on the `img` data field,\n",
        "                              # then invert `pred` based on this information. we can use same info\n",
        "                              # for multiple fields, also support different orig_keys for different fields\n",
        "            #meta_keys=\"pred_meta_dict\",  # key field to save inverted meta data, every item maps to `keys`\n",
        "            #orig_meta_keys=\"img_meta_dict\",  # get the meta data from `img_meta_dict` field when inverting,\n",
        "                                             # for example, may need the `affine` to invert `Spacingd` transform,\n",
        "                                             # multiple fields can use the same meta data to invert\n",
        "            #meta_key_postfix=\"meta_dict\",  # if `meta_keys=None`, use \"{keys}_{meta_key_postfix}\" as the meta key,\n",
        "                                           # if `orig_meta_keys=None`, use \"{orig_keys}_{meta_key_postfix}\",\n",
        "                                           # otherwise, no need this arg during inverting\n",
        "            nearest_interp=True,  # change to use \"nearest\" mode in interpolation when inverting\n",
        "            to_tensor=True,  # convert to PyTorch Tensor after inverting\n",
        "        ),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4jY3xI9xWjB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6e1LbkoDgKM",
        "outputId": "20b665a8-98c1-4cb0-e935-d19b739b624d"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "#AddChannel(), Orientation(axcodes=\"PRI\"),Flip(spatial_axis=0),\n",
        "transforms=Compose([  ToTensor()])\n",
        "check = ImageDataset(image_files=images, labels=labels,\n",
        "                        transform=transforms)\n",
        "check_loader = DataLoader(check, batch_size=1,\n",
        "                          num_workers=1, pin_memory=torch.cuda.is_available())\n",
        "for data,i in check_loader:\n",
        "  inputs = data[0].to(device)\n",
        "  #plt.imshow(data[0][:, :, 35], cmap=\"gray\")\n",
        "  print(inputs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.78125 0.78125 5.      1.     ] to [  0.78125      0.78125      5.         347.64570884]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 50, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.779297 0.779297 5.       1.      ] to [  0.77929699   0.77929699   5.         753.50438455]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 138, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.792969 0.792969 5.       1.      ] to [  0.79296899   0.79296899   5.         599.24612652]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 93, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.703125 0.703125 5.       1.      ] to [  0.703125     0.703125     5.         504.77971681]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 90, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.824219 0.824219 5.       1.      ] to [  0.82421899   0.82421899   5.         598.99445388]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 95, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.673828 0.673828 5.       1.      ] to [  0.67382801   0.67382801   5.         516.39811801]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 85, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.808 0.808 4.    1.   ] to [8.08000028e-01 8.08000028e-01 4.00000000e+00 1.42767904e+03]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 107, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.779297 0.779297 5.       1.      ] to [  0.77929699   0.77929699   5.         702.14375565]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 97, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.703125 0.703125 5.       1.      ] to [  0.703125     0.703125     5.         605.28113303]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 106, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.757812 0.757812 5.       1.      ] to [  0.75781202   0.75781202   5.         519.50800032]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 90, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.755859 0.755859 5.       1.      ] to [  0.75585902   0.75585902   5.         566.57582183]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 96, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.867188 0.867188 5.       1.      ] to [  0.86718798   0.86718798   5.         576.14641478]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 105, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.855469 0.855469 5.       1.      ] to [  0.85546899   0.85546899   5.         564.68649824]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 105, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.766 0.766 5.    1.   ] to [7.65999973e-01 7.65999973e-01 5.00000000e+00 1.34968417e+03]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 90, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.793 0.793 5.    1.   ] to [7.92999983e-01 7.92999983e-01 5.00000000e+00 1.28099597e+03]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 92, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.9 0.9 5.  1. ] to [8.99999976e-01 8.99999976e-01 5.00000000e+00 1.36386156e+03]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 95, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.919 0.919 5.    1.   ] to [9.19000030e-01 9.19000030e-01 5.00000000e+00 1.40258465e+03]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 92, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.763672 0.763672 5.       1.      ] to [  0.76367199   0.76367199   5.         677.68802741]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 89, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.881 0.881 5.    1.   ] to [8.80999982e-01 8.80999982e-01 5.00000000e+00 1.43428869e+03]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 104, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.951172 0.951172 5.       1.      ] to [  0.95117199   0.95117199   5.         730.29412781]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 92, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.934 0.934 5.    1.   ] to [9.34000015e-01 9.34000015e-01 5.00000000e+00 1.29143526e+03]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 102, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.869 0.869 4.    1.   ] to [8.69000018e-01 8.69000018e-01 4.00000000e+00 1.53970338e+03]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 116, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.781 0.781 5.    1.   ] to [7.81000018e-01 7.81000018e-01 5.00000000e+00 1.29074746e+03]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 512, 97, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtjR-k758HU7"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDn21bVzOn2w"
      },
      "source": [
        "# Create DenseNet121, CrossEntropyLoss and Adam optimizer\n",
        "device = torch.device(\"cuda\")\n",
        "model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf2I6mUmOo-5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1dfefefd-ffdb-4029-bb68-00eecd6a7c94"
      },
      "source": [
        "# start a typical PyTorch training\n",
        "val_interval = 2\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "writer = SummaryWriter()\n",
        "for epoch in range(20):\n",
        "        print(\"-\" * 10)\n",
        "        print(f\"epoch {epoch + 1}/{200}\")\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        step = 0\n",
        "        for batch_data in train_loader:\n",
        "            step += 1\n",
        "            inputs, labels = batch_data[\"img\"].to(device), batch_data[\"label\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_len = len(train_ds) // train_loader.batch_size\n",
        "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
        "            writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
        "        epoch_loss /= step\n",
        "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % val_interval == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "                y = torch.tensor([], dtype=torch.long, device=device)\n",
        "                for val_data in val_loader:\n",
        "                    val_images, val_labels = val_data[\"img\"].to(device), val_data[\"label\"].to(device)\n",
        "                    y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
        "                    y = torch.cat([y, val_labels], dim=0)\n",
        "\n",
        "                acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
        "                acc_metric = acc_value.sum().item() / len(acc_value)\n",
        "                y_onehot = to_onehot(y)\n",
        "                y_pred_act = act(y_pred)\n",
        "                auc_metric = compute_roc_auc(y_pred_act, y_onehot)\n",
        "                del y_pred_act, y_onehot\n",
        "                if acc_metric > best_metric:\n",
        "                    best_metric = acc_metric\n",
        "                    best_metric_epoch = epoch + 1\n",
        "                    torch.save(model.state_dict(),  \"/content/drive/My Drive/Spleen_AI/best_metric_model_classification3d_array.pth\")\n",
        "                    print(\"saved new best metric model\")\n",
        "                print(\n",
        "                    \"current epoch: {} current accuracy: {:.4f} current AUC: {:.4f} best accuracy: {:.4f} at epoch {}\".format(\n",
        "                        epoch + 1, acc_metric, auc_metric, best_metric, best_metric_epoch\n",
        "                    )\n",
        "                )\n",
        "                writer.add_scalar(\"val_accuracy\", acc_metric, epoch + 1)\n",
        "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
        "writer.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------\n",
            "epoch 1/200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifying image pixdim from [0.773438 0.773438 5.       1.      ] to [  0.77343798   0.77343798   5.         582.96214438]\n",
            "Modifying image pixdim from [0.976562 0.976562 3.75     1.      ] to [  0.97656202   0.97656202   3.75       890.67797211]\n",
            "Modifying image pixdim from [0.779297 0.779297 5.       1.      ] to [  0.77929699   0.77929699   5.         753.50438455]\n",
            "Modifying image pixdim from [0.824219 0.824219 5.       1.      ] to [8.24218988e-01 8.24218988e-01 5.00000000e+00 8.38878131e+02]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=== Transform input info -- Resized ===\n",
            "INFO:DataStats:\n",
            "=== Transform input info -- Resized ===\n",
            "img statistics:\n",
            "Type: <class 'numpy.ndarray'>\n",
            "Shape: (1, 512, 512, 95, 2)\n",
            "Value range: (0.0, 1.0)\n",
            "INFO:DataStats:img statistics:\n",
            "Type: <class 'numpy.ndarray'>\n",
            "Shape: (1, 512, 512, 95, 2)\n",
            "Value range: (0.0, 1.0)\n",
            "label statistics:\n",
            "Type: <class 'numpy.int64'>\n",
            "Value: 1\n",
            "INFO:DataStats:label statistics:\n",
            "Type: <class 'numpy.int64'>\n",
            "Value: 1\n",
            "img_meta_dict statistics:\n",
            "Type: <class 'dict'>\n",
            "Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  4, 512, 512,  95,   2,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(16, dtype=int16), 'bitpix': array(32, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.000000e+00, 8.242190e-01, 8.242190e-01, 5.000000e+00,\n",
            "       8.388781e+02, 1.000000e+00, 1.000000e+00, 1.000000e+00],\n",
            "      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(0, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(0, dtype=int16), 'sform_code': array(2, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(212.293, dtype=float32), 'qoffset_y': array(183.537, dtype=float32), 'qoffset_z': array(-790.545, dtype=float32), 'srow_x': array([ -0.824219,   0.      ,   0.      , 212.293   ], dtype=float32), 'srow_y': array([  0.      ,  -0.824219,   0.      , 183.537   ], dtype=float32), 'srow_z': array([   0.   ,    0.   ,    5.   , -790.545], dtype=float32), 'affine': array([[  -0.82421899,    0.        ,    0.        ,  212.29299927],\n",
            "       [   0.        ,   -0.82421899,    0.        ,  183.53700256],\n",
            "       [   0.        ,    0.        ,    5.        , -790.54498291],\n",
            "       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.82421899,    0.        ,    0.        ,  212.29299927],\n",
            "       [   0.        ,   -0.82421899,    0.        ,  183.53700256],\n",
            "       [   0.        ,    0.        ,    5.        , -790.54498291],\n",
            "       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  95], dtype=int16), 'original_channel_dim': -1, 'filename_or_obj': '/content/drive/My Drive/Spleen_AI/Projekt2f/Segmentation/13057491/13057491_seg.nii.gz'}\n",
            "\n",
            "=== Transform input info -- Resized ===\n",
            "INFO:DataStats:img_meta_dict statistics:\n",
            "Type: <class 'dict'>\n",
            "Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  4, 512, 512,  95,   2,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(16, dtype=int16), 'bitpix': array(32, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([1.000000e+00, 8.242190e-01, 8.242190e-01, 5.000000e+00,\n",
            "       8.388781e+02, 1.000000e+00, 1.000000e+00, 1.000000e+00],\n",
            "      dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(0, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(0, dtype=int16), 'sform_code': array(2, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(212.293, dtype=float32), 'qoffset_y': array(183.537, dtype=float32), 'qoffset_z': array(-790.545, dtype=float32), 'srow_x': array([ -0.824219,   0.      ,   0.      , 212.293   ], dtype=float32), 'srow_y': array([  0.      ,  -0.824219,   0.      , 183.537   ], dtype=float32), 'srow_z': array([   0.   ,    0.   ,    5.   , -790.545], dtype=float32), 'affine': array([[  -0.82421899,    0.        ,    0.        ,  212.29299927],\n",
            "       [   0.        ,   -0.82421899,    0.        ,  183.53700256],\n",
            "       [   0.        ,    0.        ,    5.        , -790.54498291],\n",
            "       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.82421899,    0.        ,    0.        ,  212.29299927],\n",
            "       [   0.        ,   -0.82421899,    0.        ,  183.53700256],\n",
            "       [   0.        ,    0.        ,    5.        , -790.54498291],\n",
            "       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  95], dtype=int16), 'original_channel_dim': -1, 'filename_or_obj': '/content/drive/My Drive/Spleen_AI/Projekt2f/Segmentation/13057491/13057491_seg.nii.gz'}\n",
            "INFO:DataStats:\n",
            "=== Transform input info -- Resized ===\n",
            "img statistics:\n",
            "Type: <class 'numpy.ndarray'>\n",
            "Shape: (1, 512, 512, 97, 2)\n",
            "Value range: (0.0, 1.0)\n",
            "INFO:DataStats:img statistics:\n",
            "Type: <class 'numpy.ndarray'>\n",
            "Shape: (1, 512, 512, 97, 2)\n",
            "Value range: (0.0, 1.0)\n",
            "label statistics:\n",
            "Type: <class 'numpy.int64'>\n",
            "Value: 0\n",
            "INFO:DataStats:label statistics:\n",
            "Type: <class 'numpy.int64'>\n",
            "Value: 0\n",
            "img_meta_dict statistics:\n",
            "Type: <class 'dict'>\n",
            "Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  4, 512, 512,  97,   2,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(16, dtype=int16), 'bitpix': array(32, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([  1.      ,   0.773438,   0.773438,   5.      , 582.96216 ,\n",
            "         1.      ,   1.      ,   1.      ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(0, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(0, dtype=int16), 'sform_code': array(2, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(196.061, dtype=float32), 'qoffset_y': array(190.037, dtype=float32), 'qoffset_z': array(-515.063, dtype=float32), 'srow_x': array([ -0.773438,   0.      ,   0.      , 196.061   ], dtype=float32), 'srow_y': array([  0.      ,  -0.773438,   0.      , 190.037   ], dtype=float32), 'srow_z': array([   0.   ,    0.   ,    5.   , -515.063], dtype=float32), 'affine': array([[  -0.77343798,    0.        ,    0.        ,  196.06100464],\n",
            "       [   0.        ,   -0.77343798,    0.        ,  190.03700256],\n",
            "       [   0.        ,    0.        ,    5.        , -515.06298828],\n",
            "       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.77343798,    0.        ,    0.        ,  196.06100464],\n",
            "       [   0.        ,   -0.77343798,    0.        ,  190.03700256],\n",
            "       [   0.        ,    0.        ,    5.        , -515.06298828],\n",
            "       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  97], dtype=int16), 'original_channel_dim': -1, 'filename_or_obj': '/content/drive/My Drive/Spleen_AI/Projekt2f/Segmentation/12789405/12789405_seg.nii.gz'}\n",
            "INFO:DataStats:img_meta_dict statistics:\n",
            "Type: <class 'dict'>\n",
            "Value: {'sizeof_hdr': array(348, dtype=int32), 'extents': array(0, dtype=int32), 'session_error': array(0, dtype=int16), 'dim_info': array(0, dtype=uint8), 'dim': array([  4, 512, 512,  97,   2,   1,   1,   1], dtype=int16), 'intent_p1': array(0., dtype=float32), 'intent_p2': array(0., dtype=float32), 'intent_p3': array(0., dtype=float32), 'intent_code': array(0, dtype=int16), 'datatype': array(16, dtype=int16), 'bitpix': array(32, dtype=int16), 'slice_start': array(0, dtype=int16), 'pixdim': array([  1.      ,   0.773438,   0.773438,   5.      , 582.96216 ,\n",
            "         1.      ,   1.      ,   1.      ], dtype=float32), 'vox_offset': array(0., dtype=float32), 'scl_slope': array(nan, dtype=float32), 'scl_inter': array(nan, dtype=float32), 'slice_end': array(0, dtype=int16), 'slice_code': array(0, dtype=uint8), 'xyzt_units': array(0, dtype=uint8), 'cal_max': array(0., dtype=float32), 'cal_min': array(0., dtype=float32), 'slice_duration': array(0., dtype=float32), 'toffset': array(0., dtype=float32), 'glmax': array(0, dtype=int32), 'glmin': array(0, dtype=int32), 'qform_code': array(0, dtype=int16), 'sform_code': array(2, dtype=int16), 'quatern_b': array(0., dtype=float32), 'quatern_c': array(0., dtype=float32), 'quatern_d': array(1., dtype=float32), 'qoffset_x': array(196.061, dtype=float32), 'qoffset_y': array(190.037, dtype=float32), 'qoffset_z': array(-515.063, dtype=float32), 'srow_x': array([ -0.773438,   0.      ,   0.      , 196.061   ], dtype=float32), 'srow_y': array([  0.      ,  -0.773438,   0.      , 190.037   ], dtype=float32), 'srow_z': array([   0.   ,    0.   ,    5.   , -515.063], dtype=float32), 'affine': array([[  -0.77343798,    0.        ,    0.        ,  196.06100464],\n",
            "       [   0.        ,   -0.77343798,    0.        ,  190.03700256],\n",
            "       [   0.        ,    0.        ,    5.        , -515.06298828],\n",
            "       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'original_affine': array([[  -0.77343798,    0.        ,    0.        ,  196.06100464],\n",
            "       [   0.        ,   -0.77343798,    0.        ,  190.03700256],\n",
            "       [   0.        ,    0.        ,    5.        , -515.06298828],\n",
            "       [   0.        ,    0.        ,    0.        ,    1.        ]]), 'as_closest_canonical': False, 'spatial_shape': array([512, 512,  97], dtype=int16), 'original_channel_dim': -1, 'filename_or_obj': '/content/drive/My Drive/Spleen_AI/Projekt2f/Segmentation/12789405/12789405_seg.nii.gz'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-e1137f31bbdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py\", line 84, in apply_transform\n    return _apply_transform(transform, data, unpack_items)\n  File \"/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py\", line 52, in _apply_transform\n    return transform(parameters)\n  File \"/usr/local/lib/python3.7/dist-packages/monai/transforms/spatial/dictionary.py\", line 541, in __call__\n    d[key] = self.resizer(d[key], mode=mode, align_corners=align_corners)\n  File \"/usr/local/lib/python3.7/dist-packages/monai/transforms/spatial/array.py\", line 387, in __call__\n    \"len(spatial_size) must be greater or equal to img spatial dimensions, \"\nValueError: len(spatial_size) must be greater or equal to img spatial dimensions, got spatial_size=3 img=4.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py\", line 84, in apply_transform\n    return _apply_transform(transform, data, unpack_items)\n  File \"/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py\", line 52, in _apply_transform\n    return transform(parameters)\n  File \"/usr/local/lib/python3.7/dist-packages/monai/transforms/compose.py\", line 159, in __call__\n    input_ = apply_transform(_transform, input_, self.map_items, self.unpack_items)\n  File \"/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py\", line 108, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.spatial.dictionary.Resized object at 0x7fa96fbbe550>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py\", line 95, in __getitem__\n    return self._transform(index)\n  File \"/usr/local/lib/python3.7/dist-packages/monai/data/dataset.py\", line 81, in _transform\n    return apply_transform(self.transform, data_i) if self.transform is not None else data_i\n  File \"/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py\", line 108, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.compose.Compose object at 0x7fa96fbbe650>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIsk3v3gKTrB"
      },
      "source": [
        "**Evaluation Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kezjNUoMVm_r"
      },
      "source": [
        "from enum import Enum\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "class Diagnosis(Enum):\n",
        "    Liver_Cirrhosis = 0\n",
        "    Lymphoma = 1\n",
        "    \n",
        "\n",
        "%matplotlib inline\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/Spleen_AI/best_metric_model_classification3d_array.pth\"))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "    y = torch.tensor([], dtype=torch.long, device=device)\n",
        "\n",
        "    for val_data in val_loader:\n",
        "              val_images, val_labels = val_data[\"img\"].to(device), val_data[\"label\"].to(device),\n",
        "              outputs = model(val_images)\n",
        "              y = torch.cat([y, val_labels], dim=0)\n",
        "              y_pred = torch.cat([y_pred, outputs.argmax(dim=1)], dim=0)\n",
        "        \n",
        "print(classification_report(\n",
        "    y.cpu().numpy(),\n",
        "    y_pred.cpu().numpy(),\n",
        "    target_names=[d.name for d in Diagnosis]))\n",
        "\n",
        "cm = confusion_matrix (\n",
        "    y.cpu().numpy(),\n",
        "    y_pred.cpu().numpy(),\n",
        "    normalize='true',\n",
        ")\n",
        "cmm = ConfusionMatrixMetric (include_background=True, metric_name=[\"sensitivity\",\"specificity\",\"negative predictive value\"])\n",
        "\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=[d.name for d in Diagnosis],\n",
        ")\n",
        "disp.plot(ax=plt.subplots(1, 1, facecolor='white')[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s6APKBEqS3E"
      },
      "source": [
        "n_examples = 5\n",
        "subplot_shape = [3, n_examples]\n",
        "fig, axes = plt.subplots(*subplot_shape, figsize=(25, 15), facecolor=\"white\")\n",
        "items = np.random.choice(len(val_ds), size=len(val_ds))\n",
        "occ_sens = monai.visualize.OcclusionSensitivity(\n",
        "    nn_module=model, mask_size=12, n_batch=10, stride=12)\n",
        "\n",
        "example = 0\n",
        "for item in items:\n",
        "\n",
        "    data = train_ds[\n",
        "        item\n",
        "    ]  # this fetches training data with random augmentations\n",
        "    image, label = data[\"img\"].unsqueeze(0).to(device), data[\"label\"]\n",
        "    img=image\n",
        "    depth_slice = img.shape[2] // 2\n",
        "    occ_sens_b_box = [-1, -1, depth_slice, depth_slice, -1, -1, -1, -1]\n",
        "\n",
        "    occ_result, _ = occ_sens(x=img, b_box=occ_sens_b_box)\n",
        "    occ_result = occ_result[..., label.item()]\n",
        "    cam_result = cam(x=img, class_idx=None)\n",
        "    cam_result = cam_result[..., depth_slice]\n",
        "    \n",
        "    name = \"actual: \"\n",
        "    \n",
        "    for row, im in enumerate([img[:, :, depth_slice, ...], occ_result, cam_result]):\n",
        "        cmap = 'gray' if row == 0 else 'jet'\n",
        "        ax = axes[row, example]\n",
        "        im_show = ax.imshow(np.squeeze(im[0][0].detach().cpu()), cmap=cmap)\n",
        "        ax.axis('off')\n",
        "        fig.colorbar(im_show, ax=ax)\n",
        "    \n",
        "    example += 1\n",
        "    if example == n_examples:\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}