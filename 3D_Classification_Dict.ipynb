{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3D_Classification_Dict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1Ahc6QaOCoako8m39k6yJEwF0n_OpNIkw",
      "authorship_tag": "ABX9TyM3r7a2r9EntaRsGoHpWYlF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meddebma/pyradiomics/blob/master/3D_Classification_Dict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiFnM05m6uL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fadffaa1-3c29-4af9-c0fb-de31248083e0"
      },
      "source": [
        "%pip install -q \"monai-weekly[itk, pillow]\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 542kB 3.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 50.1MB 110kB/s \n",
            "\u001b[K     |████████████████████████████████| 14.4MB 30.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 36.3MB 130kB/s \n",
            "\u001b[K     |████████████████████████████████| 14.0MB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 68.0MB 71kB/s \n",
            "\u001b[K     |████████████████████████████████| 10.3MB 48.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JIt-TFM-3ZL",
        "outputId": "fc499a5b-74a1-4920-d15e-34316c5545c2"
      },
      "source": [
        "# Copyright 2020 MONAI Consortium\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import pandas as pd\n",
        "import seaborn as sns \n",
        "from glob import glob\n",
        "\n",
        "import monai\n",
        "from monai.metrics import compute_roc_auc\n",
        "from monai.networks.nets import DenseNet121, Classifier\n",
        "from monai.apps import download_and_extract\n",
        "from monai.config import print_config\n",
        "from monai.utils import first\n",
        "from monai.visualize import GradCAM\n",
        "from monai.metrics import ConfusionMatrixMetric, get_confusion_matrix\n",
        "from monai.data import CacheDataset, DataLoader, ImageDataset, ITKReader, PILReader, Dataset, partition_dataset_classes\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    ConfusionMatrixDisplay, \n",
        ")\n",
        "from monai.transforms import Activations, Activationsd, AsDiscreted, AddChanneld, Invertd, AsDiscrete, Compose, LoadImaged, RandRotate90d, Resized, ScaleIntensityd, ToTensord\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "print_config()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MONAI version: 0.6.dev2124\n",
            "Numpy version: 1.19.5\n",
            "Pytorch version: 1.8.1+cu101\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
            "MONAI rev id: 35907c20d39845d8bb30bad08cf6cffa87f94389\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 3.0.2\n",
            "scikit-image version: 0.16.2\n",
            "Pillow version: 7.1.2\n",
            "Tensorboard version: 2.5.0\n",
            "gdown version: 3.6.4\n",
            "TorchVision version: 0.9.1+cu101\n",
            "ITK version: 5.1.2\n",
            "tqdm version: 4.41.1\n",
            "lmdb version: 0.99\n",
            "psutil version: 5.4.8\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvUL9l6t_tDM"
      },
      "source": [
        "**Data Direction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHeS8Rt6_zWD"
      },
      "source": [
        "data_dir= \"/content/drive/My Drive/Spleen_AI\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1S2v-B-AITM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27f8753-747a-4ae5-c20a-8254c65b11d5"
      },
      "source": [
        "labels_all = pd.read_excel (r'/content/drive/MyDrive/Spleen_AI/labels.xlsx')\n",
        "l=labels_all[\"label\"].to_numpy()\n",
        "print(l)\n",
        "#print(labels_all[\"label\"])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyjyR96rS0Bl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae44170c-c251-4c44-c227-18a69a85401b"
      },
      "source": [
        "images = sorted(glob(os.path.join(data_dir, \"Projekt2f\", \"*.nii.gz\")))\n",
        "\n",
        "#for i in images:\n",
        "#  print(os.path.basename(i))\n",
        "\n",
        "labels= l\n",
        "#labels= np.array([0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1], dtype=np.int64)\n",
        "print(len(labels))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJJwFbO47yGV"
      },
      "source": [
        "**Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ensxgO2N37j",
        "outputId": "69a6a2ef-6e20-4e89-e102-a868ebb43494"
      },
      "source": [
        "train_files = [{\"img\": img, \"label\": label} for img, label in zip(images[:100], labels[:100])]\n",
        "val_files = [{\"img\": img, \"label\": label} for img, label in zip(images[-100:], labels[-100:])]\n",
        "\n",
        "    # Define transforms for image\n",
        "train_transforms = Compose(\n",
        "        [\n",
        "            LoadImaged(keys=[\"img\"]),\n",
        "            AddChanneld(keys=[\"img\"]),\n",
        "            ScaleIntensityd(keys=[\"img\"]),\n",
        "            Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),\n",
        "            RandRotate90d(keys=[\"img\"], prob=0.8, spatial_axes=[0, 2]),\n",
        "            ToTensord(keys=[\"img\"]),\n",
        "        ]\n",
        "    )\n",
        "val_transforms = Compose(\n",
        "        [\n",
        "            LoadImaged(keys=[\"img\"]),\n",
        "            AddChanneld(keys=[\"img\"]),\n",
        "            ScaleIntensityd(keys=[\"img\"]),\n",
        "            Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),\n",
        "            ToTensord(keys=[\"img\"]),\n",
        "        ]\n",
        "    )\n",
        "act = Activations(softmax=True)\n",
        "to_onehot = AsDiscrete(to_onehot=True, n_classes=2)\n",
        "\n",
        "\n",
        "# Define dataset, data loader\n",
        "check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
        "check_loader = DataLoader(check_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n",
        "check_data = monai.utils.misc.first(check_loader)\n",
        "print(check_data[\"img\"].shape, check_data[\"label\"])\n",
        "\n",
        "# create a training data loader\n",
        "train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "# create a validation data loader\n",
        "val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "post_transforms = Compose([\n",
        "        #Activationsd(keys=\"y_pred\", sigmoid=True),\n",
        "        #AsDiscreted(keys=\"y_pred\", threshold_values=True),\n",
        "        Invertd(\n",
        "            keys=\"y_pred\",  # invert the `pred` data field, also support multiple fields\n",
        "            transform=val_transforms,\n",
        "            loader=val_loader,\n",
        "            orig_keys=\"img\",  # get the previously applied pre_transforms information on the `img` data field,\n",
        "                              # then invert `pred` based on this information. we can use same info\n",
        "                              # for multiple fields, also support different orig_keys for different fields\n",
        "            #meta_keys=\"pred_meta_dict\",  # key field to save inverted meta data, every item maps to `keys`\n",
        "            #orig_meta_keys=\"img_meta_dict\",  # get the meta data from `img_meta_dict` field when inverting,\n",
        "                                             # for example, may need the `affine` to invert `Spacingd` transform,\n",
        "                                             # multiple fields can use the same meta data to invert\n",
        "            #meta_key_postfix=\"meta_dict\",  # if `meta_keys=None`, use \"{keys}_{meta_key_postfix}\" as the meta key,\n",
        "                                           # if `orig_meta_keys=None`, use \"{orig_keys}_{meta_key_postfix}\",\n",
        "                                           # otherwise, no need this arg during inverting\n",
        "            nearest_interp=True,  # change to use \"nearest\" mode in interpolation when inverting\n",
        "            to_tensor=True,  # convert to PyTorch Tensor after inverting\n",
        "        ),\n",
        "    ])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 96, 96, 96]) tensor([1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtjR-k758HU7"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDn21bVzOn2w"
      },
      "source": [
        "# Create DenseNet121, CrossEntropyLoss and Adam optimizer\n",
        "device = torch.device(\"cuda\")\n",
        "model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-5)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf2I6mUmOo-5",
        "outputId": "1762a1f7-c7a4-4144-faf1-7a33388ba083"
      },
      "source": [
        "# start a typical PyTorch training\n",
        "val_interval = 2\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "writer = SummaryWriter()\n",
        "for epoch in range(5):\n",
        "        print(\"-\" * 10)\n",
        "        print(f\"epoch {epoch + 1}/{5}\")\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        step = 0\n",
        "        for batch_data in train_loader:\n",
        "            step += 1\n",
        "            inputs, labels = batch_data[\"img\"].to(device), batch_data[\"label\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_len = len(train_ds) // train_loader.batch_size\n",
        "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
        "            writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
        "        epoch_loss /= step\n",
        "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % val_interval == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "                y = torch.tensor([], dtype=torch.long, device=device)\n",
        "                for val_data in val_loader:\n",
        "                    val_images, val_labels = val_data[\"img\"].to(device), val_data[\"label\"].to(device)\n",
        "                    y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
        "                    y = torch.cat([y, val_labels], dim=0)\n",
        "\n",
        "                acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
        "                acc_metric = acc_value.sum().item() / len(acc_value)\n",
        "                y_onehot = to_onehot(y)\n",
        "                y_pred_act = act(y_pred)\n",
        "                auc_metric = compute_roc_auc(y_pred_act, y_onehot)\n",
        "                del y_pred_act, y_onehot\n",
        "                if acc_metric > best_metric:\n",
        "                    best_metric = acc_metric\n",
        "                    best_metric_epoch = epoch + 1\n",
        "                    torch.save(model.state_dict(), \"best_metric_model_classification3d_dict.pth\")\n",
        "                    print(\"saved new best metric model\")\n",
        "                print(\n",
        "                    \"current epoch: {} current accuracy: {:.4f} current AUC: {:.4f} best accuracy: {:.4f} at epoch {}\".format(\n",
        "                        epoch + 1, acc_metric, auc_metric, best_metric, best_metric_epoch\n",
        "                    )\n",
        "                )\n",
        "                writer.add_scalar(\"val_accuracy\", acc_metric, epoch + 1)\n",
        "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
        "writer.close()\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------\n",
            "epoch 1/5\n",
            "1/50, train_loss: 0.6393\n",
            "2/50, train_loss: 0.6776\n",
            "3/50, train_loss: 0.6279\n",
            "4/50, train_loss: 0.7164\n",
            "5/50, train_loss: 0.6433\n",
            "6/50, train_loss: 0.7373\n",
            "7/50, train_loss: 0.6969\n",
            "8/50, train_loss: 0.6914\n",
            "9/50, train_loss: 0.9545\n",
            "10/50, train_loss: 0.4852\n",
            "11/50, train_loss: 0.9488\n",
            "12/50, train_loss: 0.7156\n",
            "13/50, train_loss: 0.6865\n",
            "14/50, train_loss: 0.6919\n",
            "15/50, train_loss: 0.7127\n",
            "16/50, train_loss: 0.6993\n",
            "17/50, train_loss: 0.8415\n",
            "18/50, train_loss: 0.6985\n",
            "19/50, train_loss: 0.8215\n",
            "20/50, train_loss: 0.7010\n",
            "21/50, train_loss: 0.8323\n",
            "22/50, train_loss: 0.7132\n",
            "23/50, train_loss: 0.6036\n",
            "24/50, train_loss: 0.7356\n",
            "25/50, train_loss: 0.6866\n",
            "26/50, train_loss: 0.6987\n",
            "27/50, train_loss: 0.6274\n",
            "28/50, train_loss: 0.6684\n",
            "29/50, train_loss: 0.6713\n",
            "30/50, train_loss: 0.7131\n",
            "31/50, train_loss: 0.6996\n",
            "32/50, train_loss: 0.7128\n",
            "33/50, train_loss: 0.6027\n",
            "34/50, train_loss: 0.6835\n",
            "35/50, train_loss: 0.5464\n",
            "36/50, train_loss: 0.5643\n",
            "37/50, train_loss: 0.6934\n",
            "38/50, train_loss: 0.7235\n",
            "39/50, train_loss: 0.6581\n",
            "40/50, train_loss: 0.6352\n",
            "41/50, train_loss: 0.8149\n",
            "42/50, train_loss: 0.4730\n",
            "43/50, train_loss: 0.5902\n",
            "44/50, train_loss: 0.5385\n",
            "45/50, train_loss: 0.6996\n",
            "46/50, train_loss: 0.6744\n",
            "47/50, train_loss: 0.4454\n",
            "48/50, train_loss: 0.6756\n",
            "49/50, train_loss: 0.7065\n",
            "50/50, train_loss: 0.4452\n",
            "epoch 1 average loss: 0.6784\n",
            "----------\n",
            "epoch 2/5\n",
            "1/50, train_loss: 0.6530\n",
            "2/50, train_loss: 0.6772\n",
            "3/50, train_loss: 0.6694\n",
            "4/50, train_loss: 0.6747\n",
            "5/50, train_loss: 0.3830\n",
            "6/50, train_loss: 0.7596\n",
            "7/50, train_loss: 0.7462\n",
            "8/50, train_loss: 0.7093\n",
            "9/50, train_loss: 0.8036\n",
            "10/50, train_loss: 0.7529\n",
            "11/50, train_loss: 0.7477\n",
            "12/50, train_loss: 0.4764\n",
            "13/50, train_loss: 0.6958\n",
            "14/50, train_loss: 0.6823\n",
            "15/50, train_loss: 0.6745\n",
            "16/50, train_loss: 1.0124\n",
            "17/50, train_loss: 0.4677\n",
            "18/50, train_loss: 0.8257\n",
            "19/50, train_loss: 0.6875\n",
            "20/50, train_loss: 0.9465\n",
            "21/50, train_loss: 0.6318\n",
            "22/50, train_loss: 0.7730\n",
            "23/50, train_loss: 0.6374\n",
            "24/50, train_loss: 0.4895\n",
            "25/50, train_loss: 0.6526\n",
            "26/50, train_loss: 0.4894\n",
            "27/50, train_loss: 0.7632\n",
            "28/50, train_loss: 0.6173\n",
            "29/50, train_loss: 0.6294\n",
            "30/50, train_loss: 0.6542\n",
            "31/50, train_loss: 0.5604\n",
            "32/50, train_loss: 0.5317\n",
            "33/50, train_loss: 0.6208\n",
            "34/50, train_loss: 0.6277\n",
            "35/50, train_loss: 0.7904\n",
            "36/50, train_loss: 0.5716\n",
            "37/50, train_loss: 0.4756\n",
            "38/50, train_loss: 0.7844\n",
            "39/50, train_loss: 0.7648\n",
            "40/50, train_loss: 0.6353\n",
            "41/50, train_loss: 0.4466\n",
            "42/50, train_loss: 0.7781\n",
            "43/50, train_loss: 0.5893\n",
            "44/50, train_loss: 0.5292\n",
            "45/50, train_loss: 0.4850\n",
            "46/50, train_loss: 0.5894\n",
            "47/50, train_loss: 0.7633\n",
            "48/50, train_loss: 0.5283\n",
            "49/50, train_loss: 0.9870\n",
            "50/50, train_loss: 0.4920\n",
            "epoch 2 average loss: 0.6587\n",
            "saved new best metric model\n",
            "current epoch: 2 current accuracy: 0.7100 current AUC: 0.7377 best accuracy: 0.7100 at epoch 2\n",
            "----------\n",
            "epoch 3/5\n",
            "1/50, train_loss: 0.3581\n",
            "2/50, train_loss: 0.6537\n",
            "3/50, train_loss: 0.6943\n",
            "4/50, train_loss: 0.4280\n",
            "5/50, train_loss: 0.4615\n",
            "6/50, train_loss: 0.3856\n",
            "7/50, train_loss: 0.3864\n",
            "8/50, train_loss: 0.4039\n",
            "9/50, train_loss: 0.7047\n",
            "10/50, train_loss: 0.5539\n",
            "11/50, train_loss: 0.8067\n",
            "12/50, train_loss: 0.7868\n",
            "13/50, train_loss: 0.4790\n",
            "14/50, train_loss: 0.4826\n",
            "15/50, train_loss: 0.2845\n",
            "16/50, train_loss: 0.7848\n",
            "17/50, train_loss: 1.0907\n",
            "18/50, train_loss: 0.4039\n",
            "19/50, train_loss: 0.6547\n",
            "20/50, train_loss: 0.5622\n",
            "21/50, train_loss: 0.3461\n",
            "22/50, train_loss: 0.6862\n",
            "23/50, train_loss: 0.6391\n",
            "24/50, train_loss: 0.6231\n",
            "25/50, train_loss: 0.6187\n",
            "26/50, train_loss: 0.9117\n",
            "27/50, train_loss: 0.6337\n",
            "28/50, train_loss: 0.3223\n",
            "29/50, train_loss: 1.0753\n",
            "30/50, train_loss: 0.6950\n",
            "31/50, train_loss: 0.6615\n",
            "32/50, train_loss: 0.3304\n",
            "33/50, train_loss: 0.5036\n",
            "34/50, train_loss: 1.0156\n",
            "35/50, train_loss: 0.7934\n",
            "36/50, train_loss: 0.5372\n",
            "37/50, train_loss: 0.6048\n",
            "38/50, train_loss: 0.8062\n",
            "39/50, train_loss: 0.5362\n",
            "40/50, train_loss: 0.6180\n",
            "41/50, train_loss: 0.7997\n",
            "42/50, train_loss: 0.6105\n",
            "43/50, train_loss: 0.6243\n",
            "44/50, train_loss: 0.4952\n",
            "45/50, train_loss: 0.8134\n",
            "46/50, train_loss: 0.7939\n",
            "47/50, train_loss: 0.6686\n",
            "48/50, train_loss: 0.7880\n",
            "49/50, train_loss: 0.6906\n",
            "50/50, train_loss: 0.4943\n",
            "epoch 3 average loss: 0.6221\n",
            "----------\n",
            "epoch 4/5\n",
            "1/50, train_loss: 0.4991\n",
            "2/50, train_loss: 0.6700\n",
            "3/50, train_loss: 0.6041\n",
            "4/50, train_loss: 0.4295\n",
            "5/50, train_loss: 0.4320\n",
            "6/50, train_loss: 0.6063\n",
            "7/50, train_loss: 0.7866\n",
            "8/50, train_loss: 0.8100\n",
            "9/50, train_loss: 0.7998\n",
            "10/50, train_loss: 0.4450\n",
            "11/50, train_loss: 0.5984\n",
            "12/50, train_loss: 0.9344\n",
            "13/50, train_loss: 0.8494\n",
            "14/50, train_loss: 0.5899\n",
            "15/50, train_loss: 0.6238\n",
            "16/50, train_loss: 0.4425\n",
            "17/50, train_loss: 0.7577\n",
            "18/50, train_loss: 0.5885\n",
            "19/50, train_loss: 0.5898\n",
            "20/50, train_loss: 0.6946\n",
            "21/50, train_loss: 0.7872\n",
            "22/50, train_loss: 0.5963\n",
            "23/50, train_loss: 0.8251\n",
            "24/50, train_loss: 0.5721\n",
            "25/50, train_loss: 0.6135\n",
            "26/50, train_loss: 0.6013\n",
            "27/50, train_loss: 0.5833\n",
            "28/50, train_loss: 0.4506\n",
            "29/50, train_loss: 0.8016\n",
            "30/50, train_loss: 0.6676\n",
            "31/50, train_loss: 0.8731\n",
            "32/50, train_loss: 0.5960\n",
            "33/50, train_loss: 0.6757\n",
            "34/50, train_loss: 0.8218\n",
            "35/50, train_loss: 0.5861\n",
            "36/50, train_loss: 0.6066\n",
            "37/50, train_loss: 0.5763\n",
            "38/50, train_loss: 0.5392\n",
            "39/50, train_loss: 0.8523\n",
            "40/50, train_loss: 0.5131\n",
            "41/50, train_loss: 0.8737\n",
            "42/50, train_loss: 0.5782\n",
            "43/50, train_loss: 0.8203\n",
            "44/50, train_loss: 0.5607\n",
            "45/50, train_loss: 0.6595\n",
            "46/50, train_loss: 0.6130\n",
            "47/50, train_loss: 0.5422\n",
            "48/50, train_loss: 0.8221\n",
            "49/50, train_loss: 0.5872\n",
            "50/50, train_loss: 0.3891\n",
            "epoch 4 average loss: 0.6467\n",
            "saved new best metric model\n",
            "current epoch: 4 current accuracy: 0.7200 current AUC: 0.8348 best accuracy: 0.7200 at epoch 4\n",
            "----------\n",
            "epoch 5/5\n",
            "1/50, train_loss: 0.5494\n",
            "2/50, train_loss: 0.3712\n",
            "3/50, train_loss: 0.4020\n",
            "4/50, train_loss: 0.6017\n",
            "5/50, train_loss: 0.5895\n",
            "6/50, train_loss: 0.4297\n",
            "7/50, train_loss: 0.6052\n",
            "8/50, train_loss: 0.5237\n",
            "9/50, train_loss: 0.6574\n",
            "10/50, train_loss: 0.8190\n",
            "11/50, train_loss: 0.7110\n",
            "12/50, train_loss: 0.3965\n",
            "13/50, train_loss: 0.8046\n",
            "14/50, train_loss: 0.3433\n",
            "15/50, train_loss: 0.5816\n",
            "16/50, train_loss: 0.5324\n",
            "17/50, train_loss: 0.6206\n",
            "18/50, train_loss: 0.3210\n",
            "19/50, train_loss: 0.6873\n",
            "20/50, train_loss: 0.3386\n",
            "21/50, train_loss: 0.5855\n",
            "22/50, train_loss: 0.4193\n",
            "23/50, train_loss: 0.7969\n",
            "24/50, train_loss: 0.8776\n",
            "25/50, train_loss: 0.3392\n",
            "26/50, train_loss: 0.5992\n",
            "27/50, train_loss: 0.3470\n",
            "28/50, train_loss: 0.5366\n",
            "29/50, train_loss: 0.3007\n",
            "30/50, train_loss: 0.7353\n",
            "31/50, train_loss: 0.3644\n",
            "32/50, train_loss: 0.6114\n",
            "33/50, train_loss: 0.5700\n",
            "34/50, train_loss: 0.3956\n",
            "35/50, train_loss: 0.8996\n",
            "36/50, train_loss: 0.6003\n",
            "37/50, train_loss: 0.5763\n",
            "38/50, train_loss: 0.8107\n",
            "39/50, train_loss: 0.9182\n",
            "40/50, train_loss: 0.5695\n",
            "41/50, train_loss: 0.5914\n",
            "42/50, train_loss: 0.4351\n",
            "43/50, train_loss: 0.4096\n",
            "44/50, train_loss: 0.3913\n",
            "45/50, train_loss: 0.6927\n",
            "46/50, train_loss: 0.3483\n",
            "47/50, train_loss: 0.7697\n",
            "48/50, train_loss: 0.6489\n",
            "49/50, train_loss: 0.4621\n",
            "50/50, train_loss: 0.8033\n",
            "epoch 5 average loss: 0.5658\n",
            "train completed, best_metric: 0.7200 at epoch: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIsk3v3gKTrB"
      },
      "source": [
        "**Evaluation Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "kezjNUoMVm_r",
        "outputId": "9b14f067-2953-4ca0-dc91-640701b46506"
      },
      "source": [
        "from enum import Enum\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "class Diagnosis(Enum):\n",
        "    Liver_Cirrhosis = 0\n",
        "    Lymphoma = 1\n",
        "    \n",
        "\n",
        "%matplotlib inline\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/Spleen_AI/best_metric_model_classification3d_array.pth\"))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "    y = torch.tensor([], dtype=torch.long, device=device)\n",
        "\n",
        "    for val_data in val_loader:\n",
        "              val_images, val_labels = val_data[\"img\"].to(device), val_data[\"label\"].to(device),\n",
        "              outputs = model(val_images)\n",
        "              y = torch.cat([y, val_labels], dim=0)\n",
        "              y_pred = torch.cat([y_pred, outputs.argmax(dim=1)], dim=0)\n",
        "        \n",
        "print(classification_report(\n",
        "    y.cpu().numpy(),\n",
        "    y_pred.cpu().numpy(),\n",
        "    target_names=[d.name for d in Diagnosis]))\n",
        "\n",
        "cm = confusion_matrix (\n",
        "    y.cpu().numpy(),\n",
        "    y_pred.cpu().numpy(),\n",
        "    normalize='true',\n",
        ")\n",
        "cmm = ConfusionMatrixMetric (include_background=True, metric_name=[\"sensitivity\",\"specificity\",\"negative predictive value\"])\n",
        "\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=[d.name for d in Diagnosis],\n",
        ")\n",
        "disp.plot(ax=plt.subplots(1, 1, facecolor='white')[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Liver_Cirrhosis       0.73      0.70      0.71        43\n",
            "       Lymphoma       0.78      0.81      0.79        57\n",
            "\n",
            "       accuracy                           0.76       100\n",
            "      macro avg       0.76      0.75      0.75       100\n",
            "   weighted avg       0.76      0.76      0.76       100\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f3c801b7510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O6sxFqX7XPGb",
        "outputId": "2b916387-524e-4ac5-9450-1812cc375ce7"
      },
      "source": [
        "post_transforms = Compose([\n",
        "        #Activationsd(keys=\"y_pred\", sigmoid=True),\n",
        "        #AsDiscreted(keys=\"y_pred\", threshold_values=True),\n",
        "        Invertd(\n",
        "            keys=\"y_pred\",  # invert the `pred` data field, also support multiple fields\n",
        "            transform=val_transforms,\n",
        "            loader=val_loader,\n",
        "            orig_keys=\"img\",  # get the previously applied pre_transforms information on the `img` data field,\n",
        "                              # then invert `pred` based on this information. we can use same info\n",
        "                              # for multiple fields, also support different orig_keys for different fields\n",
        "            #meta_keys=\"pred_meta_dict\",  # key field to save inverted meta data, every item maps to `keys`\n",
        "            #orig_meta_keys=\"img_meta_dict\",  # get the meta data from `img_meta_dict` field when inverting,\n",
        "                                             # for example, may need the `affine` to invert `Spacingd` transform,\n",
        "                                             # multiple fields can use the same meta data to invert\n",
        "            #meta_key_postfix=\"meta_dict\",  # if `meta_keys=None`, use \"{keys}_{meta_key_postfix}\" as the meta key,\n",
        "                                           # if `orig_meta_keys=None`, use \"{orig_keys}_{meta_key_postfix}\",\n",
        "                                           # otherwise, no need this arg during inverting\n",
        "            nearest_interp=True,  # change to use \"nearest\" mode in interpolation when inverting\n",
        "            to_tensor=True,  # convert to PyTorch Tensor after inverting\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "with torch.no_grad():\n",
        "  for d in val_loader:\n",
        "         post_transforms(d)\n",
        "         itera = iter(d)\n",
        "\n",
        "\n",
        "\n",
        "def get_next_im():\n",
        "    test_data = next(itera)\n",
        "    return test_data[0].to(device), test_data[1].unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "def plot_occlusion_heatmap(im, heatmap):\n",
        "    plt.subplots(1, 2)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(np.squeeze(im.cpu()))\n",
        "    plt.colorbar()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(heatmap)\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=== Transform input info -- Invertd ===\n",
            "\n",
            "=== Transform input info -- Invertd ===\n",
            "INFO:DataStats:\n",
            "=== Transform input info -- Invertd ===\n",
            "img statistics:\n",
            "Type: <class 'torch.Tensor'>\n",
            "Shape: torch.Size([2, 1, 96, 96, 96])\n",
            "Value range: (0.0, 0.8472548127174377)\n",
            "img statistics:\n",
            "Type: <class 'torch.Tensor'>\n",
            "Shape: torch.Size([2, 1, 96, 96, 96])\n",
            "Value range: (0.0, 0.8472548127174377)\n",
            "INFO:DataStats:img statistics:\n",
            "Type: <class 'torch.Tensor'>\n",
            "Shape: torch.Size([2, 1, 96, 96, 96])\n",
            "Value range: (0.0, 0.8472548127174377)\n",
            "label statistics:\n",
            "Type: <class 'torch.Tensor'>\n",
            "Shape: torch.Size([2])\n",
            "Value range: (0, 1)\n",
            "label statistics:\n",
            "Type: <class 'torch.Tensor'>\n",
            "Shape: torch.Size([2])\n",
            "Value range: (0, 1)\n",
            "INFO:DataStats:label statistics:\n",
            "Type: <class 'torch.Tensor'>\n",
            "Shape: torch.Size([2])\n",
            "Value range: (0, 1)\n",
            "img_meta_dict statistics:\n",
            "Type: <class 'dict'>\n",
            "Value: {'sizeof_hdr': tensor([348, 348], dtype=torch.int32), 'extents': tensor([0, 0], dtype=torch.int32), 'session_error': tensor([0, 0], dtype=torch.int16), 'dim_info': tensor([0, 0], dtype=torch.uint8), 'dim': tensor([[  3, 512, 512, 103,   1,   1,   1,   1],\n",
            "        [  3, 512, 512, 119,   1,   1,   1,   1]], dtype=torch.int16), 'intent_p1': tensor([0., 0.]), 'intent_p2': tensor([0., 0.]), 'intent_p3': tensor([0., 0.]), 'intent_code': tensor([0, 0], dtype=torch.int16), 'datatype': tensor([4, 4], dtype=torch.int16), 'bitpix': tensor([16, 16], dtype=torch.int16), 'slice_start': tensor([0, 0], dtype=torch.int16), 'pixdim': tensor([[1.0000, 0.7720, 0.7720, 5.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.0000, 0.7820, 0.7820, 4.0000, 0.0000, 0.0000, 0.0000, 0.0000]]), 'vox_offset': tensor([0., 0.]), 'scl_slope': tensor([nan, nan]), 'scl_inter': tensor([nan, nan]), 'slice_end': tensor([0, 0], dtype=torch.int16), 'slice_code': tensor([0, 0], dtype=torch.uint8), 'xyzt_units': tensor([2, 2], dtype=torch.uint8), 'cal_max': tensor([0., 0.]), 'cal_min': tensor([0., 0.]), 'slice_duration': tensor([0., 0.]), 'toffset': tensor([0., 0.]), 'glmax': tensor([0, 0], dtype=torch.int32), 'glmin': tensor([0, 0], dtype=torch.int32), 'qform_code': tensor([2, 2], dtype=torch.int16), 'sform_code': tensor([1, 1], dtype=torch.int16), 'quatern_b': tensor([0., 0.]), 'quatern_c': tensor([0., 0.]), 'quatern_d': tensor([1., 1.]), 'qoffset_x': tensor([197.2702, 223.2418]), 'qoffset_y': tensor([197.2702, 199.8043]), 'qoffset_z': tensor([1332., 1331.]), 'srow_x': tensor([[ -0.7720,   0.0000,   0.0000, 197.2702],\n",
            "        [ -0.7820,   0.0000,   0.0000, 223.2418]]), 'srow_y': tensor([[  0.0000,  -0.7720,   0.0000, 197.2702],\n",
            "        [  0.0000,  -0.7820,   0.0000, 199.8043]]), 'srow_z': tensor([[   0.,    0.,    5., 1332.],\n",
            "        [   0.,    0.,    4., 1331.]]), 'affine': tensor([[[-7.7200e-01,  0.0000e+00,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00, -7.7200e-01,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  5.0000e+00,  1.3320e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
            "\n",
            "        [[-7.8200e-01,  0.0000e+00,  0.0000e+00,  2.2324e+02],\n",
            "         [ 0.0000e+00, -7.8200e-01,  0.0000e+00,  1.9980e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  4.0000e+00,  1.3310e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
            "       dtype=torch.float64), 'original_affine': tensor([[[-7.7200e-01,  0.0000e+00,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00, -7.7200e-01,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  5.0000e+00,  1.3320e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
            "\n",
            "        [[-7.8200e-01,  0.0000e+00,  0.0000e+00,  2.2324e+02],\n",
            "         [ 0.0000e+00, -7.8200e-01,  0.0000e+00,  1.9980e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  4.0000e+00,  1.3310e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
            "       dtype=torch.float64), 'as_closest_canonical': tensor([False, False]), 'spatial_shape': tensor([[512, 512, 103],\n",
            "        [512, 512, 119]], dtype=torch.int16), 'original_channel_dim': ['no_channel', 'no_channel'], 'filename_or_obj': ['/content/drive/My Drive/Spleen_AI/Projekt2f/12761363.nii.gz', '/content/drive/My Drive/Spleen_AI/Projekt2f/12762874.nii.gz']}\n",
            "img_meta_dict statistics:\n",
            "Type: <class 'dict'>\n",
            "Value: {'sizeof_hdr': tensor([348, 348], dtype=torch.int32), 'extents': tensor([0, 0], dtype=torch.int32), 'session_error': tensor([0, 0], dtype=torch.int16), 'dim_info': tensor([0, 0], dtype=torch.uint8), 'dim': tensor([[  3, 512, 512, 103,   1,   1,   1,   1],\n",
            "        [  3, 512, 512, 119,   1,   1,   1,   1]], dtype=torch.int16), 'intent_p1': tensor([0., 0.]), 'intent_p2': tensor([0., 0.]), 'intent_p3': tensor([0., 0.]), 'intent_code': tensor([0, 0], dtype=torch.int16), 'datatype': tensor([4, 4], dtype=torch.int16), 'bitpix': tensor([16, 16], dtype=torch.int16), 'slice_start': tensor([0, 0], dtype=torch.int16), 'pixdim': tensor([[1.0000, 0.7720, 0.7720, 5.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.0000, 0.7820, 0.7820, 4.0000, 0.0000, 0.0000, 0.0000, 0.0000]]), 'vox_offset': tensor([0., 0.]), 'scl_slope': tensor([nan, nan]), 'scl_inter': tensor([nan, nan]), 'slice_end': tensor([0, 0], dtype=torch.int16), 'slice_code': tensor([0, 0], dtype=torch.uint8), 'xyzt_units': tensor([2, 2], dtype=torch.uint8), 'cal_max': tensor([0., 0.]), 'cal_min': tensor([0., 0.]), 'slice_duration': tensor([0., 0.]), 'toffset': tensor([0., 0.]), 'glmax': tensor([0, 0], dtype=torch.int32), 'glmin': tensor([0, 0], dtype=torch.int32), 'qform_code': tensor([2, 2], dtype=torch.int16), 'sform_code': tensor([1, 1], dtype=torch.int16), 'quatern_b': tensor([0., 0.]), 'quatern_c': tensor([0., 0.]), 'quatern_d': tensor([1., 1.]), 'qoffset_x': tensor([197.2702, 223.2418]), 'qoffset_y': tensor([197.2702, 199.8043]), 'qoffset_z': tensor([1332., 1331.]), 'srow_x': tensor([[ -0.7720,   0.0000,   0.0000, 197.2702],\n",
            "        [ -0.7820,   0.0000,   0.0000, 223.2418]]), 'srow_y': tensor([[  0.0000,  -0.7720,   0.0000, 197.2702],\n",
            "        [  0.0000,  -0.7820,   0.0000, 199.8043]]), 'srow_z': tensor([[   0.,    0.,    5., 1332.],\n",
            "        [   0.,    0.,    4., 1331.]]), 'affine': tensor([[[-7.7200e-01,  0.0000e+00,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00, -7.7200e-01,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  5.0000e+00,  1.3320e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
            "\n",
            "        [[-7.8200e-01,  0.0000e+00,  0.0000e+00,  2.2324e+02],\n",
            "         [ 0.0000e+00, -7.8200e-01,  0.0000e+00,  1.9980e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  4.0000e+00,  1.3310e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
            "       dtype=torch.float64), 'original_affine': tensor([[[-7.7200e-01,  0.0000e+00,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00, -7.7200e-01,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  5.0000e+00,  1.3320e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
            "\n",
            "        [[-7.8200e-01,  0.0000e+00,  0.0000e+00,  2.2324e+02],\n",
            "         [ 0.0000e+00, -7.8200e-01,  0.0000e+00,  1.9980e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  4.0000e+00,  1.3310e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
            "       dtype=torch.float64), 'as_closest_canonical': tensor([False, False]), 'spatial_shape': tensor([[512, 512, 103],\n",
            "        [512, 512, 119]], dtype=torch.int16), 'original_channel_dim': ['no_channel', 'no_channel'], 'filename_or_obj': ['/content/drive/My Drive/Spleen_AI/Projekt2f/12761363.nii.gz', '/content/drive/My Drive/Spleen_AI/Projekt2f/12762874.nii.gz']}\n",
            "INFO:DataStats:img_meta_dict statistics:\n",
            "Type: <class 'dict'>\n",
            "Value: {'sizeof_hdr': tensor([348, 348], dtype=torch.int32), 'extents': tensor([0, 0], dtype=torch.int32), 'session_error': tensor([0, 0], dtype=torch.int16), 'dim_info': tensor([0, 0], dtype=torch.uint8), 'dim': tensor([[  3, 512, 512, 103,   1,   1,   1,   1],\n",
            "        [  3, 512, 512, 119,   1,   1,   1,   1]], dtype=torch.int16), 'intent_p1': tensor([0., 0.]), 'intent_p2': tensor([0., 0.]), 'intent_p3': tensor([0., 0.]), 'intent_code': tensor([0, 0], dtype=torch.int16), 'datatype': tensor([4, 4], dtype=torch.int16), 'bitpix': tensor([16, 16], dtype=torch.int16), 'slice_start': tensor([0, 0], dtype=torch.int16), 'pixdim': tensor([[1.0000, 0.7720, 0.7720, 5.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.0000, 0.7820, 0.7820, 4.0000, 0.0000, 0.0000, 0.0000, 0.0000]]), 'vox_offset': tensor([0., 0.]), 'scl_slope': tensor([nan, nan]), 'scl_inter': tensor([nan, nan]), 'slice_end': tensor([0, 0], dtype=torch.int16), 'slice_code': tensor([0, 0], dtype=torch.uint8), 'xyzt_units': tensor([2, 2], dtype=torch.uint8), 'cal_max': tensor([0., 0.]), 'cal_min': tensor([0., 0.]), 'slice_duration': tensor([0., 0.]), 'toffset': tensor([0., 0.]), 'glmax': tensor([0, 0], dtype=torch.int32), 'glmin': tensor([0, 0], dtype=torch.int32), 'qform_code': tensor([2, 2], dtype=torch.int16), 'sform_code': tensor([1, 1], dtype=torch.int16), 'quatern_b': tensor([0., 0.]), 'quatern_c': tensor([0., 0.]), 'quatern_d': tensor([1., 1.]), 'qoffset_x': tensor([197.2702, 223.2418]), 'qoffset_y': tensor([197.2702, 199.8043]), 'qoffset_z': tensor([1332., 1331.]), 'srow_x': tensor([[ -0.7720,   0.0000,   0.0000, 197.2702],\n",
            "        [ -0.7820,   0.0000,   0.0000, 223.2418]]), 'srow_y': tensor([[  0.0000,  -0.7720,   0.0000, 197.2702],\n",
            "        [  0.0000,  -0.7820,   0.0000, 199.8043]]), 'srow_z': tensor([[   0.,    0.,    5., 1332.],\n",
            "        [   0.,    0.,    4., 1331.]]), 'affine': tensor([[[-7.7200e-01,  0.0000e+00,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00, -7.7200e-01,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  5.0000e+00,  1.3320e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
            "\n",
            "        [[-7.8200e-01,  0.0000e+00,  0.0000e+00,  2.2324e+02],\n",
            "         [ 0.0000e+00, -7.8200e-01,  0.0000e+00,  1.9980e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  4.0000e+00,  1.3310e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
            "       dtype=torch.float64), 'original_affine': tensor([[[-7.7200e-01,  0.0000e+00,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00, -7.7200e-01,  0.0000e+00,  1.9727e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  5.0000e+00,  1.3320e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
            "\n",
            "        [[-7.8200e-01,  0.0000e+00,  0.0000e+00,  2.2324e+02],\n",
            "         [ 0.0000e+00, -7.8200e-01,  0.0000e+00,  1.9980e+02],\n",
            "         [ 0.0000e+00,  0.0000e+00,  4.0000e+00,  1.3310e+03],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
            "       dtype=torch.float64), 'as_closest_canonical': tensor([False, False]), 'spatial_shape': tensor([[512, 512, 103],\n",
            "        [512, 512, 119]], dtype=torch.int16), 'original_channel_dim': ['no_channel', 'no_channel'], 'filename_or_obj': ['/content/drive/My Drive/Spleen_AI/Projekt2f/12761363.nii.gz', '/content/drive/My Drive/Spleen_AI/Projekt2f/12762874.nii.gz']}\n",
            "img_transforms statistics:\n",
            "Type: <class 'list'>\n",
            "Value: [{'class': ['Resized', 'Resized'], 'id': tensor([139899836053392, 139899836053392]), 'orig_size': [tensor([512, 512]), tensor([512, 512]), tensor([103, 119])], 'extra_info': {'mode': ['area', 'area'], 'align_corners': ['none', 'none']}}, {'class': ['ToTensord', 'ToTensord'], 'id': tensor([139899835981712, 139899835981712]), 'orig_size': [tensor([96, 96]), tensor([96, 96]), tensor([96, 96])]}]\n",
            "img_transforms statistics:\n",
            "Type: <class 'list'>\n",
            "Value: [{'class': ['Resized', 'Resized'], 'id': tensor([139899836053392, 139899836053392]), 'orig_size': [tensor([512, 512]), tensor([512, 512]), tensor([103, 119])], 'extra_info': {'mode': ['area', 'area'], 'align_corners': ['none', 'none']}}, {'class': ['ToTensord', 'ToTensord'], 'id': tensor([139899835981712, 139899835981712]), 'orig_size': [tensor([96, 96]), tensor([96, 96]), tensor([96, 96])]}]\n",
            "INFO:DataStats:img_transforms statistics:\n",
            "Type: <class 'list'>\n",
            "Value: [{'class': ['Resized', 'Resized'], 'id': tensor([139899836053392, 139899836053392]), 'orig_size': [tensor([512, 512]), tensor([512, 512]), tensor([103, 119])], 'extra_info': {'mode': ['area', 'area'], 'align_corners': ['none', 'none']}}, {'class': ['ToTensord', 'ToTensord'], 'id': tensor([139899835981712, 139899835981712]), 'orig_size': [tensor([96, 96]), tensor([96, 96]), tensor([96, 96])]}]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/monai/transforms/post/dictionary.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mkey_iterator\u001b[0;34m(self, data, *extra_iterables)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_missing_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key was missing ({key}) and allow_missing_keys==False\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'Key was missing (y_pred) and allow_missing_keys==False'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4b7103574d4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m          \u001b[0mpost_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m          \u001b[0mitera\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/monai/transforms/compose.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_transform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0m_log_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"applying transform {transform}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.post.dictionary.Invertd object at 0x7f3c8211bd10>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "vfFYizJOg1TG",
        "outputId": "dda87bf2-b64c-4d97-ff61-57b0d18148a9"
      },
      "source": [
        "# Get a random image and its corresponding label\n",
        "img, label = get_next_im()\n",
        "\n",
        "# Get the occlusion sensitivity map\n",
        "occ_sens = monai.visualize.OcclusionSensitivity(\n",
        "    nn_module=model, mask_size=12, n_batch=10, stride=12)\n",
        "# Get the CAM\n",
        "cam = monai.visualize.GradCAM(\n",
        "    nn_module=model, target_layers=\"class_layers.relu\"\n",
        ")\n",
        "# Only get a single slice to save time.\n",
        "# For the other dimensions (channel, width, height), use\n",
        "# -1 to use 0 and img.shape[x]-1 for min and max, respectively\n",
        "depth_slice = img[0][0]\n",
        "\n",
        "occ_sens_b_box = [-1, -1, depth_slice, depth_slice, -1, -1, -1, -1]\n",
        "\n",
        "occ_result, _ = occ_sens(x=img, b_box=occ_sens_b_box)\n",
        "occ_result = occ_result[..., label.item()]\n",
        "cam_result = cam(x=img, class_idx=None)\n",
        "cam_result = cam_result[..., depth_slice]\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(25, 15), facecolor='white')\n",
        "\n",
        "for i, im in enumerate([img[:, :, depth_slice, ...], occ_result, cam_result]):\n",
        "    cmap = 'gray' if i == 0 else 'jet'\n",
        "    ax = axes[i]\n",
        "    if isinstance(im, torch.Tensor):\n",
        "        im = im.cpu().detach()    \n",
        "    #im_show = ax.imshow(im[0][0], cmap=cmap)\n",
        "    im_show = ax.imshow(np.squeeze(im[0][0]), cmap=cmap)\n",
        "    ax.axis('off')\n",
        "    fig.colorbar(im_show, ax=ax)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-05cee4e469f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# For the other dimensions (channel, width, height), use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# -1 to use 0 and img.shape[x]-1 for min and max, respectively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdepth_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mocc_sens_b_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWaSsjkTXoly"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}